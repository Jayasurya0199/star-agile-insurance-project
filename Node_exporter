1. Install & Run Node Exporter on Master & Worker
Download & Extract
On both the master and the node machines, SSH in and run:


wget https://github.com/prometheus/node_exporter/releases/download/v1.7.0/node_exporter-1.7.0.linux-amd64.tar.gz
tar xvf node_exporter-1.7.0.linux-amd64.tar.gz
sudo mv node_exporter-1.7.0.linux-amd64/node_exporter /usr/local/bin/

-------------------------------------------------------------------------------------------------------------------------------------
2. Create a node_exporter Service

Create vi /etc/systemd/system/node_exporter.service with:


[Unit]
Description=Prometheus Node Exporter
Wants=network-online.target
After=network-online.target

[Service]
User=root
ExecStart=/usr/local/bin/node_exporter

[Install]
WantedBy=default.target
Enable & Start

-----------------------------------------------------------------------------------------------------------------------------------------------------
sudo systemctl daemon-reload
sudo systemctl enable node_exporter
sudo systemctl start node_exporter
sudo systemctl status node_exporter   # verify it’s running
By default it listens on port 9100.
----------------------------------------------------------------------------------------------------------------------------------------------------------------
2. Configure Prometheus to Scrape Both Exporters

Edit Prometheus’ config (on your Prometheus server, typically at /etc/prometheus/prometheus.yml):

global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'k8s-nodes'
    static_configs:
      - targets: ['MASTER_IP:9100','WORKER_IP:9100']
Replace MASTER_IP and WORKER_IP with the actual node IPs.

Reload Prometheus


sudo systemctl reload prometheus
or, if reload isn’t supported:


sudo systemctl restart prometheus
Verify in Prometheus

Browse to http://<PROMETHEUS_IP>:9090/targets

You should see two healthy targets under k8s-nodes.
----------------------------------------------------------------------------------------------------------------------------------------------------------------

3. Add Prometheus as a Datasource in Grafana
Log in to Grafana at http://<GRAFANA_IP>:3000 (default admin/admin).

Add Data Source

In the left menu, click Configuration → Data Sources → Add data source.

Choose Prometheus.

URL: http://<PROMETHEUS_IP>:9090

Click Save & Test — you should see “Data source is working”.
----------------------------------------------------------------------------------------------------------------------------------------------------------------

4. Import a Node Exporter Dashboard
In Grafana, click Create → Import.

Enter Dashboard ID 1860 (official Node Exporter Full) or 1861 (Node Exporter for Kubernetes).

Click Load, select your Prometheus data source, then Import.

You’ll now have detailed CPU, memory, filesystem, network, and more metrics for both nodes.

Recap
Node Exporter on each machine (9100).

Prometheus scrape those exporters via prometheus.yml.

Grafana uses Prometheus as a datasource and imports a Node Exporter dashboard.
----------------------------------------------------------------------------------------------------------------------------------------------------------------

| **Metric**                       | **What It Shows**                                     | **Example PromQL Query**                                                                                                                             |
| -------------------------------- | ----------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| **CPU Usage**                    | Percentage of CPU being used by a node                | `100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`                                                                   |
| **Memory Utilization**           | % of RAM in use vs total                              | `(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100`                                                                          |
| **Disk I/O Throughput**          | Read/write bytes per second                           | `rate(node_disk_io_time_seconds_total[1m])`                                                                                                          |
| **Disk Space Usage**             | % of filesystem used                                  | `(node_filesystem_size_bytes{fstype!="rootfs"} - node_filesystem_free_bytes{fstype!="rootfs"}) / node_filesystem_size_bytes{fstype!="rootfs"} * 100` |
| **Network Traffic (Throughput)** | Bytes in/out per second                               | `rate(node_network_receive_bytes_total[1m])`, `rate(node_network_transmit_bytes_total[1m])`                                                          |
| **Pod Count per Node**           | Number of pods running on each node                   | `count(kube_pod_info) by (node)`                                                                                                                     |
| **Container Restarts**           | How many times containers have restarted (error sign) | `increase(kube_pod_container_status_restarts_total[1h])`                                                                                             |
| **Node Disk Pressure**           | Node under disk pressure condition                    | `kube_node_status_condition{condition="DiskPressure", status="true"}`                                                                                |
| **API Server Request Rate**      | Requests per second to the Kubernetes API server      | `sum(rate(rest_client_requests_total[5m])) by (code)`                                                                                                |
| **Kubelet HTTP Requests Errors** | Error rate from kubelet                               | `sum(rate(kubelet_http_requests_total{code=~"5.."}[5m])) by (instance)`                                                                              |
